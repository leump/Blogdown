<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet"> 
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.3/css/all.css" integrity="sha384-UHRtZLI+pbxtHCWp1t77Bi1L4ZtiqrqD80Kn4Z8NTSRyMA2Fd33n5dQ8lWUE00s/" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"> 
    
    <link rel="stylesheet" href="../../../../fonts/academicons-1.8.6/css/academicons.min.css"/>
    <link rel="icon" type="image/png" sizes="32x32" href="../../../../logo/bodhi.png"> 
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    
    
    <title>Visual World Paradigm - An eye-tracking technique  - Shixiang Wang</title>
    
     
    <meta property="og:title" content="Visual World Paradigm - Shixiang, Wang | 王诗翔">
    

    
      
    

    

    
    

    <link rel="stylesheet" href="../../../../css/style.css" />
    <link rel="stylesheet" href="../../../../css/mystyle.css" /> 
    <link rel="stylesheet" href="../../../../css/fonts.css" />
    
<link rel="stylesheet" href="../../../../css/custom.css" />

  </head>

  
  <body class="en">
    <header class="masthead">
      

<h1><a href="../../../../"><img src="https://avatars1.githubusercontent.com/u/25057508?s=460&amp;v=4" alt="Shixiang Wang" /></a></h1>



      <nav class="menu">
        <input id="menu-check" type="checkbox" />
        <label id="menu-label" for="menu-check" class="unselectable">
          <span class="icon close-icon">✕</span>
          <span class="icon open-icon">☰</span>
          <span class="text">Menu</span>
        </label>
        <ul>
        
        
        <li><a href="../../../../">Home</a></li>
        
        <li><a href="../../../../en/about/">About</a></li>
        
        <li><a href="../../../../en/cv-en/lzhanen">CV</a></li>
        
        <li><a href="../../../../en/teach/">Teach</a></li>
        
        <li><a href="../../../../en/post/">Blog</a></li>
        
        <li><a href="../../../../en/paradigm/">Paradigm</a></li>
        
        <li><a href="../../../../cn/">中文</a></li>
        
        

<li class="meu-extra"></li>






<li><a href="https://github.com/ShixiangWang/home/edit/master/content/en/paradigm/Language/Visual-World-Paradigm.Rmd" target="_blank">Edit Me</a></li>


<li><a href="../../../../en/index.xml" type="application/rss+xml" title="RSS feed">Subscribe</a></li>

<li><a href="http://creativecommons.org/licenses/by-nc-sa/4.0/" title="Attribution-NonCommercial-ShareAlike 4.0 International">License</a></li>


        </ul>
      </nav>
    </header>

    <article class="main">
      <header class="title">
        

<h1>Visual World Paradigm</h1>
<h1><span class="subtitle">An eye-tracking technique</span></h2>


<h3>Likan Zhan &middot 
2018-02-07</h3> 


   
  


      </header>




<div id="TOC">
<ul>
<li><a href="#a-brief-description">1. A brief description</a></li>
<li><a href="#recent-applications">2. Recent Applications</a></li>
<li><a href="#references">3. References</a></li>
</ul>
</div>

<div id="a-brief-description" class="section level1">
<h1>1. A brief description</h1>
<p>This paradigm relies on two seminal work published by <span class="citation">Cooper (<a href="#ref-RN906">1974</a>)</span> and by <span class="citation">Tanenhaus, Spivey-Knowlton, Eberhard, &amp; Sedivy (<a href="#ref-RN907">1995</a>)</span>. In a typical eye tracking study using the visual world paradigm, participants’ eye movements to objects or pictures in the visual workspace are recorded via an eye tracker as the participant produces or comprehends a spoken language describing the concurrent visual world. This paradigm has high versatility, as it can be used in a wide range of populations, including those who cannot read and/or who cannot overtly give their behavioral responses, such as preliterate children, elderly adults, and patients. More importantly, the paradigm is extremely sensitive to fine grained manipulations of the speech signal, and it can be used to study the online processing of most topics in language comprehension at multiple levels, such as the fine grained acoustic phonetic features, the properties of words, and the linguistic structures.</p>
</div>
<div id="recent-applications" class="section level1">
<h1>2. Recent Applications</h1>
<ol style="list-style-type: decimal">
<li><span class="citation">Groot, Huettig, &amp; Olivers (<a href="#ref-RN913">2017</a>)</span></li>
</ol>
<p>On all trials, participants memorized a spoken word for a verbal recognition test at the end of the trial. During the retention period, they performed a visual search task. In crucial trials, the search target were absent. In a crucial trial, for example, the word to remember was “banana”. They then saw four object printed on the screen. These contained an object that was semantically related (such as the monkey), an object that was visually related (such as the canoe), and two objects that were unrelated (such as the hat and the tambourine). In the visual search stage, participant were asked to search the banana (the template condition) or the figurine (Accessory condition). The article observed that participants’ eye movements are significantly different between the accessory condition and the template condition, suggesting that language-induced attentional biases are subject to task requirements.</p>
<ol start="2" style="list-style-type: decimal">
<li><span class="citation">Saryazdi &amp; Chambers (<a href="#ref-RN931">2018</a>)</span></li>
</ol>
<p>To explore the effects of the degree of image realism, researchers conducted two eye tracking studies using the visual world paradigm. The test image consist of four objects, such as a cigarette, a banana, an earings, and an apple. The test images consist of both the phorographs and the clipart images of the same objects. The two experiments differ in whether the test audios are noun-biased (Experiment 1), such as <em>John will move the apple/banana</em>, or verb-biased (Experiment 2), such as <em>John will move/peal the apple/banana</em>. Researchers found a modest benefit for clipart stimuli during real-time processing, but only for noun-driving mappings, i.e., the effect of realism was observed in experiment 1 but not in experiment 2.</p>
<ol start="3" style="list-style-type: decimal">
<li><span class="citation">Kreysa, Nunnemann, &amp; Knoeferle (<a href="#ref-RN1009">2018</a>)</span></li>
</ol>
<p>The authors monitored participants’ eye movements to mentioned characters while they listened to transitive sentences. They varied whether speaker gaze, a depicted action, neither, or both of these visual cues were available, as well as whether both cues we deictic(Experiment 1) or only speaker gaze(Experiment 2). Speaker gaze affected eye movements during comprehension similarly early to a single deictic action depiction, but significantly earlier than non-deictic action depictions; conversely, depicted actions but not speaker gaze positively affected later recall of sentence content.</p>
<ol start="4" style="list-style-type: decimal">
<li><span class="citation">Thothathiri, Asaro, Hsu, &amp; Novick (<a href="#ref-RN1010">2018</a>)</span></li>
</ol>
<p>Figuring out <em>who did what to whom</em> is a critical component in sentence comprehension. This so called <em>themantic role assignment</em> is influenced by both syntactic and semantic cues. Conflict between these cues can result in temporary consideration of multiple incompatible interpretations during real-time sentence processing. The authors conduced two stuides to test whether the resolution of syntax-semantics conflict can be expedited by the online engagement of cognitive control processes that are routinely used to regulate behavior across domains.</p>
<p>In the two experiments, critical stroop-sentence pairs consisted of a stroop trial (trial <em>n-1</em>) followed by a sentence trial (trial <em>n</em>). In the Stroop task, participants viewed words on a computer screen and indicated the font colors of the words, such as <span style="color:blue">blue</span> (congruent), <span style="color:blue">red</span> (incongruent). On sentence comprehension trials, participants hear a sentence and select a picture from four options, such as <em>the rabbit was chased by the fox</em> (congruent) or <em>the fox was chased by the rabbit</em> (incongruent).</p>
<p>The results showed that the prior incongruent stroop trial can faliciate the resolution of syntax-semantics conflict in the sentence comprehension trial, reflected in a) fewer looks to a picture illustrating the competing but incorrect interpretation (Experiment 1), and b) steeper growth in looks to a picture illustrating the correct interpretation (Experiment 2).</p>
<ol start="5" style="list-style-type: decimal">
<li><span class="citation">Yamashiro &amp; Vouloumanos (<a href="#ref-RN1011">2018</a>)</span></li>
</ol>
<p>Speech allows humans to communicate and to navigate the social world. The authors reported an visual world experiment to explore whether infants, like adults, process communicative events while the event is occurring. In their experiment, infants saw a sequence of three trials in a pre-recorded video of a third-party communicative interaction. During the familiarization trials, the Communicator looked at two novel objects and grasped the target object. During the action segment of the test trial, the Communicator could no longer reach the objects, so she vocalized to the Listener using speech or cough. The Listener selected either the target object or the non-target object. In the still image segment, the final image of the test trial froze for the remainder of the test trial. Areas of interest used in all trials are shown in the image of the still image segment of the test trial. The results showed that children by 12 months, like adults can immediately evaluate the communicator’s speech, but not her cough, as communicative and recognized that the Listener should select the target object only when the Communicator spoke.</p>
<ol start="6" style="list-style-type: decimal">
<li><span class="citation">McMurray, Danelz, Rigler, &amp; Seedorff (<a href="#ref-RN1012">2018</a>)</span></li>
</ol>
<p>The authors reported an visual-world study exploring the development of <em>speech categorization</em> from children to adolescence. Children from 3 age groups (7–8, 12–13, and 17–18 years) heard a token from either a b/p or s/􏰀 continua spanning 2 words (beach/peach, ship/sip) and selected its referent from a screen containing 4 pictures of potential lexical candidates. Eye movements to each object were monitored as a measure of how strongly children were committing to each candidate as perception unfolds in real-time. Results showed an ongoing sharpening of speech categories through 18, which was particularly apparent during the early stages of real-time perception.</p>
<ol start="6" style="list-style-type: decimal">
<li><span class="citation">Lowder &amp; Ferreira (<a href="#ref-RN1017">2018</a>)</span></li>
</ol>
<p>Participants eye-movement on a picture consisting of a reparandum(e.g., cat), a repair (e.g., a dog), and two unrelated distractors (e.g., a plant and a dishtowel), were monitored as they were listening the test sentences.</p>
<p>The contextual plausibility of the misspoken word and the certainty with which the speaker uttered this word were systematically manipulated:</p>
<ul>
<li><ol style="list-style-type: lower-alpha">
<li>Every Saturday, Bill likes to grab a book and sit on the couch with his cat, uh I mean his dog, where they spend the afternoon. (Plausible-Certain)</li>
</ol></li>
<li><ol start="2" style="list-style-type: lower-alpha">
<li>Every Saturday, Bill likes to grab a Frisbee and go to the park with his cat, uh I mean his dog, where they spend the afternoon. (Implausible-Certain)</li>
</ol></li>
<li><ol start="3" style="list-style-type: lower-alpha">
<li>Every Saturday, Bill likes to grab a book and sit on the couch with his uh cat, uh I mean his dog, where they spend the afternoon. (Plausible-Uncertain)</li>
</ol></li>
<li><ol start="4" style="list-style-type: lower-alpha">
<li>Every Saturday, Bill likes to grab a Frisbee and go to the park with his uh cat, uh I mean his dog, where they spend the afternoon. (Implausible-Uncertain)</li>
</ol></li>
</ul>
<p>Results showed that listeners immediately exploited these cues to generate top-down expectations regarding the speaker’s communicative intention. Crucially, listeners used these expectations to constrain the bottom-up speech input and mentally correct perceived speech errors, even before the speaker initiated the correction.</p>
</div>
<div id="references" class="section level1 unnumbered">
<h1>3. References</h1>
<div id="refs" class="references">
<div id="ref-RN906">
<p>Cooper, R. M. (1974). The control of eye fixation by the meaning of spoken language: A new methodology for the real-time investigation of speech perception, memory, and language processing. <em>Cognitive Psychology</em>, <em>6</em>(1), 84–107. Journal Article. doi:<a href="https://doi.org/10.1016/0010-0285(74)90005-x">10.1016/0010-0285(74)90005-x</a></p>
</div>
<div id="ref-RN913">
<p>Groot, F. de, Huettig, F., &amp; Olivers, C. N. L. (2017). Language-induced visual and semantic biases in visual search are subject to task requirements. <em>Visual Cognition</em>, <em>25</em>(1-3), 225–240. Journal Article. doi:<a href="https://doi.org/10.1080/13506285.2017.1324934">10.1080/13506285.2017.1324934</a></p>
</div>
<div id="ref-RN1009">
<p>Kreysa, H., Nunnemann, E. M., &amp; Knoeferle, P. (2018). Distinct effects of different visual cues on sentence comprehension and later recall: The case of speaker gaze versus depicted actions. <em>Acta Psychologica</em>, <em>188</em>, 220–229. Journal Article. doi:<a href="https://doi.org/10.1016/j.actpsy.2018.05.001">10.1016/j.actpsy.2018.05.001</a></p>
</div>
<div id="ref-RN1017">
<p>Lowder, M. W., &amp; Ferreira, F. (2018). I see what you meant to say: Anticipating speech errors during online sentence processing. <em>Journal of Experimental Psychology: General</em>. Journal Article. doi:<a href="https://doi.org/10.1037/xge0000544">10.1037/xge0000544</a></p>
</div>
<div id="ref-RN1012">
<p>McMurray, B., Danelz, A., Rigler, H., &amp; Seedorff, M. (2018). Speech categorization develops slowly through adolescence. <em>Developmental Psychology</em>, <em>54</em>(8), 1472–1491. Journal Article. doi:<a href="https://doi.org/10.1037/dev0000542">10.1037/dev0000542</a></p>
</div>
<div id="ref-RN931">
<p>Saryazdi, R., &amp; Chambers, C. G. (2018). Mapping language to visual referents: Does the degree of image realism matter? <em>Acta Psychologica</em>, <em>182</em>, 91–99. Journal Article. doi:<a href="https://doi.org/10.1016/j.actpsy.2017.11.003">10.1016/j.actpsy.2017.11.003</a></p>
</div>
<div id="ref-RN907">
<p>Tanenhaus, M. K., Spivey-Knowlton, M. J., Eberhard, K. M., &amp; Sedivy, J. C. (1995). Integration of visual and linguistic information in spoken language comprehension. <em>Science</em>, <em>268</em>(5217), 1632–1634. Journal Article. doi:<a href="https://doi.org/10.1126/science.7777863">10.1126/science.7777863</a></p>
</div>
<div id="ref-RN1010">
<p>Thothathiri, M., Asaro, C. T., Hsu, N. S., &amp; Novick, J. M. (2018). Who did what ? A causal role for cognitive control in thematic role assignment during sentence comprehension. <em>Cognition</em>, <em>178</em>, 162–177. Journal Article. doi:<a href="https://doi.org/10.1016/j.cognition.2018.05.014">10.1016/j.cognition.2018.05.014</a></p>
</div>
<div id="ref-RN1011">
<p>Yamashiro, A., &amp; Vouloumanos, A. (2018). How do infants and adults process communicative events in real time? <em>Journal of Experimental Child Psychology</em>, <em>173</em>, 268–283. Journal Article. doi:<a href="https://doi.org/10.1016/j.jecp.2018.04.011">10.1016/j.jecp.2018.04.011</a></p>
</div>
</div>
</div>


  <footer>
  
<nav class="post-nav">
  <span class="nav-prev"></span>
  <span class="nav-next"></span>
</nav>
<script type="text/javascript">
document.addEventListener('keyup', function(e) {
  if (e.target.nodeName.toUpperCase() != 'BODY') return;
  var url = false;
  if (e.which == 37) {  
    
  } else if (e.which == 39) {  
    
  }
  if (url) window.location = url;
});
</script>



<section class="comments">
  <div id="disqus_thread"></div>
  <script src="../../../../js/disqusloader.min.js"></script>
  <script>
  var disqus_config = function () {
  
    this.page.url = "https:\/\/github.com\/ShixiangWang" + location.pathname;
  
  };
  (function() {
    var inIFrame = function() {
      var iframe = true;
      try { iframe = window.self !== window.top; } catch (e) {}
      return iframe;
    };
    if (inIFrame()) return;
    var disqus_js = '//home-xjdzylqrzp.disqus.com/embed.js';
    
    if (location.hash.match(/^#comment/)) {
      var d = document, s = d.createElement('script');
      s.src = disqus_js; s.async = true;
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    } else {
      disqusLoader('#disqus_thread', {
        scriptUrl: disqus_js, laziness: 0, disqusConfig: disqus_config
      });
    }
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>




<script async src="../../../../js/center-img.js"></script>

<script async src="../../../../js/right-quote.js"></script>

<script async src="../../../../js/no-highlight.js"></script>

<script async src="../../../../js/fix-footnote.js"></script>


<script async src="//cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>

  


  
  
  

  <div class="copyright"><a href="mailto:w_shixiang@163.com"><i class='far fa-envelope fa-1x'></i></a> &middot; <a href="https://github.com/ShixiangWang"><i class='fab fa-github fa-1x'></i></a> &middot; <a href="https://stackoverflow.com/users/7662327/shixiang-wang"><i class='fab fa-stack-overflow fa-1x'></i></a> &middot; <a href="https://scholar.google.com/citations?user=FvNp0NkAAAAJ&amp;hl=en"><i class='ai ai-google-scholar ai-1x'></i></a> &middot; <a href="https://orcid.org/0000-0002-9275-3557"><i class='ai ai-orcid ai-1x'></i></a> &middot; <a href="https://www.researchgate.net/profile/Wang_Shixiang4"><i class='ai ai-researchgate ai-1x'></i></a> <br> Powered by <a href="https://gohugo.io">Hugo</a> and <a href="https://bookdown.org/yihui/blogdown/">Blogdown</a> &copy; <a href="../../../../">Shixiang Wang</a> 2019 </div>
  
  

  <script type="text/javascript" src="//rf.revolvermaps.com/0/0/6.js?i=56h9es09xn7&amp;m=6&amp;c=ff0000&amp;cr1=ffffff&amp;f=arial&amp;l=1" async="async"></script>
  </footer>
  </article>
  
  </body>
</html>

